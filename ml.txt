https://drive.google.com/drive/folders/1b_O1m0r14n6ww6xNyP5MTaCeMVmEMWH0?usp=sharing

___________________________
Linear regression

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Load the dataset
df=pd.read_csv(r'/content/placement.csv')
df

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Load the dataset
df=pd.read_csv(r'/content/placement.csv')
df


# Features (CGPA) and Target (Package)
x = df.iloc[:, 0:1]
y = df.iloc[:, -1]

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

# Linear Regression model
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(x_train, y_train)

# Predictions on the test data
y_predict = lr.predict(x_test)
# Print predicted values
y_predict

# Scatter plot and regression line
plt.scatter(df['cgpa'], df['package'])
plt.plot(x_train, lr.predict(x_train), color='red')
plt.xlabel('CGPA')
plt.ylabel('Package')



# Slope (m) and Intercept (b) of the regression line
m = lr.coef_
print("Slope, m = ", m)
b = lr.intercept_
print("Y-Intercept, c = ", b)

# Predicted values using the regression equation
print("Prediction for CGPA 8.58:", m * 8.58 + b)
print("Prediction for CGPA 6.86:", m * 6.86 + b)


# Calculate Mean Squared Error (MSE)
from sklearn.metrics import mean_squared_error
MSE = mean_squared_error(y_test, y_predict)
print("Mean Squared Error is:", MSE)

# Calculate Root Mean Squared Error (RMSE)
RMSE = np.sqrt(MSE)
print("Root Mean Squared Error is:", RMSE)

# Calculate Mean Absolute Error (MAE)
from sklearn.metrics import mean_absolute_error
MAE = mean_absolute_error(y_test, y_predict)
print("Mean Absolute Error is:", MAE)

_____________________________________
KMeans

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset("iris")
df.head()


x = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]

from sklearn.cluster import KMeans

wcss = []
for i in range(1, 11):
    km = KMeans(n_clusters=i)
    km.fit(x)
    wcss.append(km.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.xticks(range(1, 11))
plt.show()



from sklearn.cluster import KMeans

km = KMeans(n_clusters=3)
z = x.values
# print(z)

y = km.fit_predict(z)
print(y)

plt.scatter(z[y==0, 0], z[y==0, 1], color='blue')
plt.scatter(z[y==1, 0], z[y==1, 1], color='red')
plt.scatter(z[y==2, 0], z[y==2, 1], color='green')

plt.show()

_____________________________________
3 KNN

from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', version=1, as_frame=False)


import pandas as pd
mnist

x = mnist.data
y = mnist.target

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

for i in range(1, 11):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(x_train, y_train)
    y_pred = knn.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("The Accuracy for K =", i, "is =", accuracy)



from sklearn.decomposition import PCA
pca = PCA(n_components=2)
x_pca = pca.fit_transform(x)
x_pca



y_pred = y.astype(int)
import matplotlib.pyplot as plt
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=y_pred, cmap='rainbow')
plt.show()

_____________________________________
4.decision tree

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix

iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target

X  # Display X (first few rows)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

dt = DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_split=2, min_samples_leaf=3, max_leaf_nodes=5, min_impurity_decrease=0.01, max_features=None, random_state=42)
dt.fit(X_train, y_train)

y_pred = dt.predict(X_test)
y_pred  # Display y_pred

accuracy = accuracy_score(y_test, y_pred)
print(f'Decision Tree Accuracy: {accuracy * 100:.2f}%')

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

print('confusion matrix')
print(confusion_matrix(y_test, y_pred))

plt.figure(figsize=(10, 6))
plot_tree(dt, filled=True, feature_names=iris.feature_names[:2], class_names=iris.target_names)


_____________________________________
5.ann to do bank churn customer prediction


from google.colab import files
upload = files.upload()

import numpy as np
import pandas as pd

df = pd.read_csv('Churn_Modelling.csv')

df.head()
df.info()
print(df.columns)

print(df['Gender'].value_counts())
print(df['Geography'].value_counts())

df = pd.get_dummies(df, columns=['Gender', 'Geography'])

x = df.drop(columns=['RowNumber', 'CustomerId', 'Surname', 'Exited'])
y = df['Exited']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

from sklearn.metrics import accuracy_score

print(accuracy_score(y_test, y_pred))


import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])



_____________________________________
6.ann on mnist data

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

x_train.shape
x_test.shape
y_train

x_train[0]

import matplotlib.pyplot as plt
plt.imshow(x_train[10])

x_train = x_train / 255  # Max Normalization
x_test = x_test / 255

x_train[0]


model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(128, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(10, activation='softmax'))

model.summary()

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=25, validation_split=0.3)

y_prob = model.predict(x_test)
y_pred = y_prob.argmax(axis=1)


from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])


plt.imshow(x_test[1])

model.predict(x_test[1].reshape(1, 28, 28)).argmax(axis=1)

_____________________________________
7.multinomial naive baye's


from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd


email_data = pd.read_csv('/enron_spam_data.csv')

print(email_data.columns)

# Map 'ham' to 0 and 'spam' to 1 in the 'label' column
email_data['label'] = email_data['Spam/Ham'].map({'ham': 0, 'spam': 1})

# Separate features  (X) and target (y)
y = email_data['label']

# Import CountVectorizer for text feature extraction
from sklearn.feature_extraction.text import CountVectorizer

# **Handle missing values in the 'Message' column**
# Replace NaN values with an empty string
email_data['Message'] = email_data['Message'].fillna('')

# Initialize and fit CountVectorizer with a maximum of 5000 features
vectorizer = CountVectorizer(max_features=65000)
X = vectorizer.fit_transform(email_data['Message']).toarray()

# Print the shape of the feature matrix (optional)
print(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the Multinomial Naive Bayes classifier
nbc = MultinomialNB()
nbc.fit(X_train, y_train)

# Make predictions on the test set
y_pred = nbc.predict(X_test)

# Calculate and print the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
_____________________________________
